# Pulse Ops Node - Next.js Agent

Agente de monitoreo construido con Next.js 16 y OpenTelemetry para recolectar mÃ©tricas de sistema (CPU, RAM).

**Arquitectura de cluster distribuido**: El agente tiene un **nodo Cassandra local** que:
- **Se une al cluster central** como miembro (rack4)
- **Recibe replicaciones automÃ¡ticas** vÃ­a Gossip Protocol
- **Permite escrituras locales** que se sincronizan automÃ¡ticamente
- **Alta disponibilidad** con RF=3 (datos en 3+ nodos siempre)

## ğŸ—ï¸ Arquitectura del Agente

### Flujo de datos completo (cluster distribuido)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ AGENT MACHINE (Edge Node)                                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    OTLP/gRPC (4317)   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Next.js App     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚ Local Collector  â”‚  â”‚
â”‚  â”‚  (pulse-ops-node)â”‚    localhost:4317      â”‚  (OTel Collector)â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â€¢ CPU metrics                                       â”‚             â”‚
â”‚   â€¢ RAM metrics                                       â”‚             â”‚
â”‚   â€¢ Customer labels                                   â”‚             â”‚
â”‚                                                       â”‚             â”‚
â”‚                                            OTLP/gRPC  â”‚             â”‚
â”‚                                            (primary)  â”‚             â”‚
â”‚                                                       â”‚             â”‚
â”‚                                                       â–¼             â”‚
â”‚                                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                                          â”‚ Cassandra Agent    â”‚    â”‚
â”‚                                          â”‚ (rack4)            â”‚    â”‚
â”‚                                          â”‚ â€¢ Cluster member   â”‚    â”‚
â”‚                                          â”‚ â€¢ Seeds: 1,2,3     â”‚    â”‚
â”‚                                          â”‚ â€¢ Port: 9043       â”‚    â”‚
â”‚                                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                    â”‚                â”‚
â”‚                                                    â”‚ Gossip         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚ Protocol
                                                     â”‚ (Replication)
                                                     â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚ CENTRAL INFRASTRUCTURE              â”‚
                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                              â”‚  â”‚ Central Collectorâ”‚              â”‚
                              â”‚  â”‚  (port 4317)     â”‚              â”‚
                              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                              â”‚           â”‚                         â”‚
                              â”‚           â–¼                         â”‚
                              â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
                              â”‚  â”‚ Cassandra Cluster            â”‚  â”‚
                              â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚  â”‚
                              â”‚  â”‚ â”‚ Node 1 â”‚  â”‚ Node 2 â”‚       â”‚  â”‚
                              â”‚  â”‚ â”‚(rack1) â”‚  â”‚(rack2) â”‚       â”‚  â”‚
                              â”‚  â”‚ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜       â”‚  â”‚
                              â”‚  â”‚     â”‚ Gossip     â”‚           â”‚  â”‚
                              â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚  â”‚
                              â”‚  â”‚            â”‚                 â”‚  â”‚
                              â”‚  â”‚       â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”            â”‚  â”‚
                              â”‚  â”‚       â”‚ Node 3  â”‚            â”‚  â”‚
                              â”‚  â”‚       â”‚(rack3)  â”‚            â”‚  â”‚
                              â”‚  â”‚       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚  â”‚
                              â”‚  â”‚            â”‚                 â”‚  â”‚
                              â”‚  â”‚            â”‚ Gossip +        â”‚  â”‚
                              â”‚  â”‚            â”‚ Replication     â”‚  â”‚
                              â”‚  â”‚            â”‚                 â”‚  â”‚
                              â”‚  â”‚       â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚  â”‚
                              â”‚  â”‚       â”‚cassandra-agentâ”‚      â”‚  â”‚
                              â”‚  â”‚       â”‚    (rack4)    â”‚â—„â”€â”€â”€â”€â”€â”¼â”€â”€â”¤
                              â”‚  â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚  â”‚
                              â”‚  â”‚       Connected to cluster   â”‚  â”‚
                              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                              â”‚                                     â”‚
                              â”‚  Keyspace: pulseops                â”‚
                              â”‚  RF=3 (NetworkTopologyStrategy)    â”‚
                              â”‚  Datos en 3+ nodos siempre         â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

NOTA: Cassandra Agent ES MIEMBRO del cluster central
      ReplicaciÃ³n automÃ¡tica vÃ­a Gossip Protocol (RF=3)
```

### Comportamiento en diferentes escenarios

#### âœ… Escenario 1: Todo conectado (normal)

1. **App Next.js** â†’ mÃ©tricas â†’ **Collector Local** (localhost:4317)
2. **Collector Local** â†’ **Central Collector** (primary path) âœ…
3. **Central Collector** â†’ escribe a **Cassandra Cluster** (cualquier nodo)
4. **Cassandra Gossip** replica automÃ¡ticamente a todos los nodos (incluido agente)
5. Resultado: Datos en **3+ nodos** (RF=3), incluyendo el nodo del agente

#### ğŸ”Œ Escenario 2: Central offline (red desconectada)

1. **App Next.js** â†’ mÃ©tricas â†’ **Collector Local** âœ…
2. **Collector Local** intenta exportar a **Central Collector** âŒ (falla conexiÃ³n)
3. **Persistent queue** guarda mÃ©tricas en disco (retry automÃ¡tico)
4. **Collector Local** puede escribir localmente a **Cassandra Agent** (opcional)
5. Cuando la conexiÃ³n vuelve:
   - **Persistent queue** envÃ­a mÃ©tricas acumuladas al central
   - **Central** escribe al cluster
   - **Gossip** replica a todos los nodos (sincronizaciÃ³n automÃ¡tica)
6. Resultado: **Consistencia eventual** garantizada por Cassandra

#### ğŸ’¾ Escenario 3: Nodo agente offline (falla local)

1. **App Next.js** â†’ mÃ©tricas â†’ **Collector Local** âœ…
2. **Collector Local** â†’ **Central Collector** âœ… (path siempre disponible)
3. **Central** â†’ escribe a **Cassandra Cluster** (nodos centrales)
4. **Nodo agente caÃ­do** â†’ NO recibe replicaciones temporalmente
5. Cuando el agente vuelve:
   - **Gossip Protocol** detecta el nodo
   - **Hinted handoff** y **read repair** sincronizan datos perdidos
   - **Consistencia eventual** restaurada automÃ¡ticamente
6. Resultado: Sin pÃ©rdida de datos, sincronizaciÃ³n automÃ¡tica

### Ventajas de esta arquitectura

| Ventaja | DescripciÃ³n |
|---------|-------------|
| **ğŸ›¡ï¸ Alta disponibilidad** | RF=3 + 1 nodo agente = 4 nodos con datos completos |
| **ğŸ”„ ReplicaciÃ³n automÃ¡tica** | Gossip Protocol sincroniza todos los nodos sin configuraciÃ³n manual |
| **âš¡ Lecturas locales** | El agente puede leer de su nodo local sin latencia de red |
| **ğŸ“Š Consistencia eventual** | Cassandra garantiza sincronizaciÃ³n automÃ¡tica (hinted handoff, read repair) |
| **ğŸ¯ DistribuciÃ³n geogrÃ¡fica** | Nodos agente en edge + cluster central = arquitectura multi-regiÃ³n natural |
| **ğŸ“ˆ Escalabilidad** | Agregar agentes = agregar nodos al cluster (scaling horizontal) |
| **ğŸ’¾ Sin doble escritura** | Una sola escritura se replica automÃ¡ticamente (no hay duplicados) |

### Componentes del agente

1. **Next.js App** (`pulse-ops-node`):
   - Genera mÃ©tricas de sistema (CPU, RAM)
   - EnvÃ­a vÃ­a OpenTelemetry SDK a `localhost:4317`
   
2. **Local Collector** (`otel-collector`):
   - Recibe OTLP/gRPC en puerto 4317
   - Aplica procesamiento (batch, attributes, filters)
   - **Export primario**: Central Collector (con retry + persistent queue)

3. **Cassandra Agent Node**:
   - **Miembro del cluster** PulseOpsCluster (rack4)
   - **Seeds**: cassandra-1, cassandra-2, cassandra-3
   - **ReplicaciÃ³n automÃ¡tica** vÃ­a Gossip Protocol
   - **Lecturas locales** rÃ¡pidas para el agente
   - Puerto: 9043 (externo), 9042 (interno cluster)

3. **Cassandra Adapter** (solo en failover):
   - Recibe mÃ©tricas del collector vÃ­a HTTP POST
   - Transforma a schema Cassandra (`pulseops.metrics`)
   - Inserta en Cassandra Agent Node

4. **Cassandra Agent Node** (standalone):
   - Nodo Cassandra **independiente** (NO cluster)
   - Almacena datos localmente (solo durante failover)
   - **SimpleStrategy, RF=1** (nodo Ãºnico)
   - NO se replica al cluster central (son storages separados)

## ğŸ“‹ Prerequisitos

- Node.js 18+
- Docker + Docker Compose
- **Cassandra Cluster** corriendo (ver `services/storage/cassandra/`)
- **OpenTelemetry Collector Central** corriendo (opcional si quieres modo standalone)

## ğŸš€ Inicio RÃ¡pido

### OpciÃ³n 1: Desarrollo local (sin Docker)

```bash
# Instalar dependencias
npm install

# Configurar variables de entorno
cp .env.local.example .env.local
# Editar .env.local con tus valores

# Desarrollo
npm run dev
```

**Nota**: En modo dev, la app envÃ­a a `localhost:4317`. Necesitas:
- Local Collector corriendo, o
- Cambiar `OTEL_EXPORTER_OTLP_ENDPOINT` para apuntar directamente al central

### OpciÃ³n 2: Stack completo con Docker Compose (recomendado)

```bash
# 1. Levantar Cassandra Cluster (central)
cd ../../storage/cassandra
docker compose up -d

# 2. Levantar Cassandra Agent Node
cd ../../agent/cassandra
docker compose up -d

# 3. Levantar Local Collector + Adapter + App (TODO: crear compose completo)
cd ../pulse-ops-node
docker compose up -d

# Ver logs del agente
docker compose logs -f pulse-ops-node

# Ver logs del collector local
docker compose logs -f otel-collector-local
```

### OpciÃ³n 3: Solo app en Docker (sin collector local)

```bash
# Build the image
docker build -t pulse-ops-node .

# Run pointing directly to central collector
docker run -p 3001:3001 \
  -e OTEL_EXPORTER_OTLP_ENDPOINT=http://host.docker.internal:4317 \
  -e OTEL_SERVICE_NAME=pulse-ops-node \
  -e CUSTOMER_ID=customer-123 \
  pulse-ops-node
```

**âš ï¸ Sin collector local = sin resiliencia offline**

## ğŸ“Š MÃ©tricas Recolectadas

- `system.cpu.percent` - Porcentaje de uso de CPU
- `system.memory.percent` - Porcentaje de uso de RAM

Las mÃ©tricas se exportan cada **5 segundos** al OpenTelemetry Collector vÃ­a OTLP/gRPC.

## ğŸ”§ ConfiguraciÃ³n

### Variables de entorno de la aplicaciÃ³n

Archivo `.env.local` (desarrollo) o variables en Docker Compose (producciÃ³n):

```env
# OpenTelemetry - Apunta al collector LOCAL (no al central)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317

# IdentificaciÃ³n del servicio
OTEL_SERVICE_NAME=pulse-ops-node
CUSTOMER_ID=customer-123

# Entorno
NODE_ENV=production
```

### ConfiguraciÃ³n del Local Collector (TODO)

El collector local (`otel-collector-config.yaml`) debe tener **failover exporters**:

**Receivers**:
- `otlp`: gRPC en puerto 4317 (recibe de la app)

**Processors** (mismo que central):
- `batch`: Agrupa mÃ©tricas (5000 metrics, 10s timeout)
- `attributes`: Agrega labels (customer_id, node_id)
- `resource`: Detecta hostname, OS, etc.

**Exporters con Failover**:
```yaml
exporters:
  # PRIMARY: Central Collector
  otlp/central:
    endpoint: http://host.docker.internal:4317
    tls:
      insecure: true
    retry_on_failure:
      enabled: true
      initial_interval: 5s
      max_interval: 30s
      max_elapsed_time: 5m
    sending_queue:
      enabled: true
      num_consumers: 2
      queue_size: 5000
      storage: file_storage  # Persistent queue
  
  # FALLBACK: Cassandra Adapter (solo si central falla)
  otlphttp/cassandra-adapter:
    endpoint: http://cassandra-adapter:8080/metrics
    timeout: 5s

# File storage para persistent queue
extensions:
  file_storage:
    directory: /var/lib/otelcol/file_storage
    timeout: 10s

# Pipeline con failover
service:
  extensions: [file_storage]
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch, attributes, resource]
      exporters: [otlp/central]  # Solo primary en pipeline normal
      
      # Nota: El failover a cassandra-adapter se activa mediante
      # configuraciÃ³n del exporter, NO en el pipeline.
      # OpenTelemetry NO tiene soporte nativo para failover exporters.
      # Alternativa: Usar 2 pipelines con routing processor.
```

**ImplementaciÃ³n de Failover** (requiere configuraciÃ³n avanzada):

OpciÃ³n A: Usar **routing processor** con health_check:
```yaml
processors:
  routing:
    from_attribute: fallback_mode  # Set by health_check
    table:
      - value: "true"
        exporters: [otlphttp/cassandra-adapter]
    default_exporters: [otlp/central]
```

OpciÃ³n B: Usar **2 collectors** en cascade (mÃ¡s simple):
- Collector1: App â†’ OTLP â†’ file queue â†’ Collector2
- Collector2: Lee queue â†’ intenta central â†’ si falla â†’ Cassandra

**RecomendaciÃ³n**: Usar **OpciÃ³n B** (2 collectors) por simplicidad.

### Cassandra Adapter (TODO)

PequeÃ±o servicio HTTP que recibe mÃ©tricas OTLP y escribe a Cassandra:

```typescript
// POST /metrics
{
  "resourceMetrics": [{
    "resource": { "attributes": [{"key": "customer.id", "value": "customer-123"}] },
    "scopeMetrics": [{
      "metrics": [{
        "name": "system.cpu.percent",
        "gauge": { "dataPoints": [{"timeUnixNano": "...", "asDouble": 45.5}] }
      }]
    }]
  }]
}
```

Transforma a:
```sql
INSERT INTO pulseops.metrics (node_id, metric_name, time_bucket, timestamp, value)
VALUES ('pulse-ops-node', 'system.cpu.percent', '2025-11-04', '2025-11-04 12:34:56', 45.5);
```

## ğŸ§ª Verificar que Funciona

### Logs esperados en la app Next.js

Al ejecutar `npm run dev` o `docker compose up`:

```log
ğŸš€ Inicializando OpenTelemetry...
âœ… OpenTelemetry inicializado correctamente
   ğŸ“¡ Collector: http://localhost:4317
   ğŸ·ï¸  Service: pulse-ops-node
   ğŸ‘¤ Customer: customer-123
ğŸ“Š Registrando mÃ©tricas del sistema...
âœ… MÃ©tricas del sistema registradas (CPU, RAM)
[12:34:56] ğŸ“Š system.cpu.percent: 45.23%
[12:34:56] ğŸ“Š system.memory.percent: 68.91%
```

Cada **5 segundos** verÃ¡s nuevos logs con valores actualizados.

### Verificar collector local recibe mÃ©tricas

```bash
# Ver logs del collector local
docker compose logs -f otel-collector-local

# DeberÃ­as ver:
# 2025-11-04T12:34:56.123Z  info  exporterhelper/queued_retry.go  Exporting
# 2025-11-04T12:34:56.456Z  info  otlp/exporter.go  Sent metrics to central
```

### Verificar datos en Cassandra Agent

```bash
# Conectar al nodo Cassandra del agente
docker exec -it pulseops-agent-cassandra cqlsh

# Query reciente
SELECT * FROM pulseops.metrics 
WHERE node_id='pulse-ops-node' 
  AND metric_name='system.cpu.percent' 
  AND time_bucket='2025-11-04'
LIMIT 10;

# DeberÃ­as ver filas con timestamps recientes
```

### Verificar replicaciÃ³n al cluster central

```bash
# Conectar al nodo central
docker exec -it pulseops-db-1 cqlsh

# Misma query (datos deben estar replicados)
SELECT * FROM pulseops.metrics 
WHERE node_id='pulse-ops-node' 
  AND metric_name='system.cpu.percent' 
  AND time_bucket='2025-11-04'
LIMIT 10;
```

### Probar resiliencia (offline mode)

```bash
# 1. Detener el collector central
cd ../../../../collector
docker compose down

# 2. App sigue enviando al local collector
docker compose -f ../agent/pulse-ops-node/docker-compose.yml logs -f

# 3. Verificar que datos se guardan en Cassandra local
docker exec -it pulseops-agent-cassandra cqlsh -e "
  SELECT COUNT(*) FROM pulseops.metrics 
  WHERE node_id='pulse-ops-node' 
    AND metric_name='system.cpu.percent' 
    AND time_bucket='2025-11-04';
"

# 4. Reiniciar collector central
docker compose up -d

# 5. Ver logs: el collector local enviarÃ¡ datos acumulados (retry queue)
docker compose -f ../agent/pulse-ops-node/docker-compose.yml logs otel-collector-local
# DeberÃ­as ver: "Retrying batch send" y luego "Successfully sent"
```

## ğŸš€ PrÃ³ximos Pasos (ImplementaciÃ³n)

Para completar esta arquitectura, necesitamos crear:

### 1. Local Collector
- [ ] `services/agent/collector/config/otel-collector-config.yaml`
- [ ] `services/agent/collector/docker-compose.yml`
- [ ] Configurar receivers, processors, exporters (dual)

### 2. Cassandra Adapter
- [ ] `services/agent/adapter/src/index.ts` (HTTP server + Cassandra client)
- [ ] `services/agent/adapter/Dockerfile`
- [ ] `services/agent/adapter/package.json`

### 3. Docker Compose Unificado
- [ ] Actualizar `services/agent/pulse-ops-node/docker-compose.yml`
- [ ] Incluir: app + collector + adapter + Cassandra node
- [ ] Configurar networks y dependencias

### 4. Testing
- [ ] Probar flujo completo: app â†’ collector â†’ dual export
- [ ] Probar modo offline y recovery
- [ ] Verificar replicaciÃ³n Cassandra

**Â¿Quieres que implemente estos componentes ahora?** ğŸš€

Puedo crear:
- Collector config con retry queue + dual exporters
- Adapter HTTP â†’ Cassandra (Node.js con cassandra-driver)
- Docker compose completo con todos los servicios
- Scripts de testing para verificar resiliencia

---

## ğŸ“Š MÃ©tricas Recolectadas

- `system.cpu.percent` - Porcentaje de uso de CPU (ObservableGauge con delta)
- `system.memory.percent` - Porcentaje de uso de RAM (ObservableGauge)

Las mÃ©tricas se exportan cada **5 segundos** al Local Collector vÃ­a OTLP/gRPC.

**Labels automÃ¡ticos**:
- `customer.id`: ID del cliente (multi-tenant)
- `service.name`: Nombre del servicio (`pulse-ops-node`)
- `host.name`: Hostname del agente
- `node.id`: ID Ãºnico del nodo generado automÃ¡ticamente

## ğŸ³ Docker Deployment (ProducciÃ³n)

La imagen usa **standalone output** con multi-stage builds:
- TamaÃ±o final: **~110MB** (vs ~1GB sin standalone)
- Base: `node:20-alpine`
- User: non-root `nodejs` (seguridad)

Puerto expuesto: **3001**

La aplicaciÃ³n corre en <http://localhost:3001>.

