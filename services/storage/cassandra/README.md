# Cassandra Cluster - Storage Backend

Cluster de 3 nodos Cassandra 5.0 para almacenamiento de mÃ©tricas histÃ³ricas (cold storage).

**âœ¨ ConfiguraciÃ³n completamente automatizada:**
- âœ… Todo en `docker-compose.yml` (sin archivos externos)
- âœ… Sin Python, sin scripts, sin Dockerfile custom
- âœ… InicializaciÃ³n automÃ¡tica de keyspace y tablas
- âœ… 1 solo comando: `docker compose up -d`

## ðŸš€ Quick Start

```bash
# Iniciar cluster completo (automÃ¡ticamente crea keyspace y tablas)
docker compose up -d

# Ver logs de inicializaciÃ³n
docker compose logs -f cassandra-init

# Verificar estado del cluster
docker exec -it pulseops-db-1 nodetool status

# Verificar que se creÃ³ el keyspace
docker exec -it pulseops-db-1 cqlsh -e "DESCRIBE KEYSPACE pulseops"

# Conectar con cqlsh
docker exec -it pulseops-db-1 cqlsh
```

## ðŸ“Š Arquitectura del Cluster

### ConfiguraciÃ³n actual:
- **VersiÃ³n**: Cassandra 5.0 (imagen oficial)
- **Nodos**: 3 nodos (pulseops-db-1, pulseops-db-2, pulseops-db-3)
- **Datacenter**: `dc1` (datacenter Ãºnico)
- **Racks**: rack1, rack2, rack3 (distribuciÃ³n de rÃ©plicas)
- **Replication Factor**: 3 (cada dato en los 3 nodos)
- **Cluster Name**: `PulseOpsCluster`
- **Snitch**: GossipingPropertyFileSnitch

### Recursos por nodo:
- **Memoria lÃ­mite**: 3GB por contenedor
- **Heap Java**: NewSize 256M, MaxSize 1G
- **Puerto**: 9042 (solo expuesto en cassandra-1)
- **Red**: cassandra-net (bridge network)

## ðŸ—„ï¸ Schema

### Keyspace: `pulseops`
- Replication Factor: 3
- Strategy: NetworkTopologyStrategy

### Tabla: `metrics`
```cql
PRIMARY KEY ((node_id, metric_name, time_bucket), timestamp)
```

Particionamiento por:
- `node_id`: ID del agente/nodo
- `metric_name`: Nombre de la mÃ©trica (system.cpu.percent, etc.)
- `time_bucket`: DÃ­a ('2025-11-04')

Clustering por:
- `timestamp`: Orden descendente (mÃ¡s reciente primero)

### Optimizaciones

- **TimeWindowCompactionStrategy**: Ventana de 1 dÃ­a
- **TTL**: 30 dÃ­as por defecto
- **GC Grace**: 1 dÃ­a
- **Ãndice secundario** en labels para filtrar por customer_id

## ðŸ”§ ConfiguraciÃ³n

### Healthchecks
```yaml
interval: 30s
timeout: 10s
retries: 10
start_period: 60s  # Dar tiempo al cluster para iniciar
```

### Recursos
```yaml
memory: 3g per node
HEAP_NEWSIZE: 256M
MAX_HEAP_SIZE: 1G
```

## ðŸ“ Queries Comunes

### Insertar una mÃ©trica
```cql
INSERT INTO pulseops.metrics (node_id, metric_name, time_bucket, timestamp, value)
VALUES ('agent-001', 'system.cpu.percent', '2025-11-04', toTimestamp(now()), 45.5);
```

### Consultar mÃ©tricas de un nodo en un dÃ­a
```cql
SELECT * FROM pulseops.metrics 
WHERE node_id = 'agent-001' 
  AND metric_name = 'system.cpu.percent'
  AND time_bucket = '2025-11-04'
LIMIT 100;
```

### Ãšltimas 10 mÃ©tricas (mÃ¡s recientes)
```cql
SELECT timestamp, value FROM pulseops.metrics 
WHERE node_id = 'agent-001' 
  AND metric_name = 'system.memory.percent'
  AND time_bucket = '2025-11-04'
LIMIT 10;
```

### Consultar rango de tiempo especÃ­fico
```cql
SELECT * FROM pulseops.metrics 
WHERE node_id = 'agent-001'
  AND metric_name = 'system.cpu.percent'
  AND time_bucket = '2025-11-04'
  AND timestamp >= '2025-11-04 10:00:00'
  AND timestamp <= '2025-11-04 12:00:00';
```

## ðŸ” Comandos de Monitoreo

### Estado del cluster
```bash
# Ver estado de todos los nodos
docker exec -it pulseops-db-1 nodetool status

# Resultado esperado:
# UN = Up + Normal (todos los nodos deben estar UN)
# 100.0% = Cada nodo tiene todos los datos (RF=3)
```

### InformaciÃ³n del ring
```bash
# Ver distribuciÃ³n de tokens
docker exec -it pulseops-db-1 nodetool ring

# Ver informaciÃ³n del cluster
docker exec -it pulseops-db-1 nodetool describecluster
```

### Schema y datos
```bash
# Ver todos los keyspaces
docker exec -it pulseops-db-1 cqlsh -e "DESCRIBE KEYSPACES"

# Ver schema completo de pulseops
docker exec -it pulseops-db-1 cqlsh -e "DESCRIBE KEYSPACE pulseops"

# Ver estadÃ­sticas de la tabla metrics
docker exec -it pulseops-db-1 nodetool tablestats pulseops.metrics

# Contar registros (cuidado en producciÃ³n)
docker exec -it pulseops-db-1 cqlsh -e "SELECT COUNT(*) FROM pulseops.metrics"
```

### Logs de contenedores
```bash
# Ver logs de un nodo especÃ­fico
docker compose logs cassandra-1

# Ver logs de inicializaciÃ³n
docker compose logs cassandra-init

# Seguir logs en tiempo real
docker compose logs -f cassandra-1
```

## ðŸ› ï¸ Troubleshooting

### Cluster no se forma
```bash
# Verificar logs de cada nodo
docker compose logs cassandra-1
docker compose logs cassandra-2
docker compose logs cassandra-3

# Verificar conectividad
docker exec -it pulseops-db-1 nodetool describecluster
```

### Reiniciar cluster limpio
```bash
docker compose down -v  # âš ï¸ ESTO ELIMINA TODOS LOS DATOS
docker compose up -d
```

### Agregar mÃ¡s nodos
Simplemente duplica el servicio en docker-compose.yml:
```yaml
cassandra-4:
  image: cassandra:5.0
  environment:
    - CASSANDRA_SEEDS=cassandra-1,cassandra-2,cassandra-3,cassandra-4
    # ... resto de config
```

## ðŸ“ˆ CaracterÃ­sticas de Rendimiento

### Capacidades del cluster actual:
- **Escrituras**: ~10,000-30,000 ops/sec (combinado en 3 nodos)
- **Lecturas**: Depende del partition key y consistency level
  - Con partition key completa: < 10ms
  - Sin partition key (full scan): muy lento, evitar
- **Compaction**: SizeTieredCompactionStrategy (automÃ¡tica)
- **TTL automÃ¡tico**: 30 dÃ­as (2,592,000 segundos)
- **Consistency Level**: Default LOCAL_ONE para lecturas/escrituras

### LÃ­mites de diseÃ±o:
- **Max partition size**: ~100MB (evitar particiones grandes)
- **Time bucket**: Particionamiento por dÃ­a (balancear carga)
- **Replication Factor**: 3 (todas las escrituras en 3 nodos)

### Best Practices:
1. **Siempre especificar partition key completa** en queries:
   ```cql
   WHERE node_id = ? AND metric_name = ? AND time_bucket = ?
   ```
2. **Usar LIMIT** en queries para evitar timeouts
3. **Evitar SELECT ***: Especificar columnas necesarias
4. **Monitoring**: Revisar `nodetool tablestats` regularmente

## ðŸ”— IntegraciÃ³n con el Stack de Monitoreo

Este cluster Cassandra es el **cold storage** del sistema PulseOps:

```
OpenTelemetry Agent (port 3001)
    â†“ OTLP gRPC
OpenTelemetry Collector (port 4317)
    â†“
    â”œâ”€â†’ Prometheus (hot: 30 dÃ­as)     port 9090
    â””â”€â†’ Cassandra (cold: histÃ³rico)   port 9042
```

### Flujo de datos:
1. **Agent** (Next.js) envÃ­a mÃ©tricas vÃ­a OTLP gRPC
2. **Collector** recibe y procesa mÃ©tricas
3. **Prometheus** almacena Ãºltimos 30 dÃ­as (queries rÃ¡pidas)
4. **Cassandra** almacena histÃ³rico completo (queries analÃ­ticas)

### PrÃ³ximos pasos:
- [ ] Crear adapter OTLP â†’ Cassandra
- [ ] Configurar remote write desde Prometheus
- [ ] Implementar agregaciones pre-calculadas
- [ ] Dashboard Grafana con fuente Cassandra
